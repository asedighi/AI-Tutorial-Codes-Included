{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, re, time, textwrap, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "def sh(cmd, check=True, env=None, cwd=None):\n",
        "    print(f\"$ {cmd}\")\n",
        "    p = subprocess.run(cmd, shell=True, env={**os.environ, **(env or {})} if env else None,\n",
        "                       cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(p.stdout)\n",
        "    if check and p.returncode!=0: raise RuntimeError(p.stdout)\n",
        "    return p.stdout"
      ],
      "metadata": {
        "id": "E63D-qxLLw8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORK=Path(\"/content/mle_colab_demo\"); WORK.mkdir(parents=True, exist_ok=True)\n",
        "PROJ=WORK/\"proj\"; PROJ.mkdir(exist_ok=True)\n",
        "DATA=WORK/\"data.csv\"; MODEL=WORK/\"model.joblib\"; PREDS=WORK/\"preds.csv\"\n",
        "SAFE=WORK/\"train_safe.py\"; RAW=WORK/\"agent_train_raw.py\"; FINAL=WORK/\"train.py\"\n",
        "MODEL_NAME=os.environ.get(\"OLLAMA_MODEL\",\"llama3.2:1b\")\n",
        "\n",
        "sh(\"pip -q install --upgrade pip\")\n",
        "sh(\"pip -q install mle-agent==0.4.* scikit-learn pandas numpy joblib\")\n",
        "\n",
        "sh(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "sv = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(4); sh(f\"ollama pull {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "xy1haaUwLyBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "np.random.seed(0)\n",
        "n=500; X=np.random.rand(n,4); y=(X@np.array([0.4,-0.2,0.1,0.5])+0.15*np.random.randn(n)>0.55).astype(int)\n",
        "pd.DataFrame(np.c_[X,y], columns=[\"f1\",\"f2\",\"f3\",\"f4\",\"target\"]).to_csv(DATA, index=False)\n",
        "\n",
        "env = {\"OPENAI_API_KEY\":\"\", \"ANTHROPIC_API_KEY\":\"\", \"GEMINI_API_KEY\":\"\",\n",
        "       \"OLLAMA_HOST\":\"http://127.0.0.1:11434\", \"MLE_LLM_ENGINE\":\"ollama\",\"MLE_MODEL\":MODEL_NAME}\n",
        "prompt=f\"\"\"Return ONE fenced python code block only.\n",
        "Write train.py that reads {DATA}; 80/20 split (random_state=42, stratify);\n",
        "Pipeline: SimpleImputer + StandardScaler + LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42);\n",
        "Print ROC-AUC & F1; print sorted coefficient magnitudes; save model to {MODEL} and preds to {PREDS};\n",
        "Use only sklearn, pandas, numpy, joblib; no extra text.\"\"\"\n",
        "def extract(txt:str)->str|None:\n",
        "    txt=re.sub(r\"\\x1B\\[[0-?]*[ -/]*[@-~]\", \"\", txt)\n",
        "    m=re.search(r\"```(?:python)?\\s*([\\s\\S]*?)```\", txt, re.I)\n",
        "    if m: return m.group(1).strip()\n",
        "    if txt.strip().lower().startswith(\"python\"): return txt.strip()[6:].strip()\n",
        "    m=re.search(r\"(?:^|\\n)(from\\s+[^\\n]+|import\\s+[^\\n]+)([\\s\\S]*)\", txt);\n",
        "    return (m.group(1)+m.group(2)).strip() if m else None\n",
        "\n",
        "out = sh(f'printf %s \"{prompt}\" | mle chat', check=False, cwd=str(PROJ), env=env)\n",
        "code = extract(out) or sh(f'printf %s \"{prompt}\" | ollama run {MODEL_NAME}', check=False, env=env)\n",
        "code = extract(code) if code and not isinstance(code, str) else (code or \"\")\n",
        "(Path(RAW)).write_text(code or \"\", encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "1jIhIZYIL5Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize(src:str)->str:\n",
        "    if not src: return \"\"\n",
        "    s = src\n",
        "    s = re.sub(r\"\\r\",\"\",s)\n",
        "    s = re.sub(r\"^python\\b\",\"\",s.strip(), flags=re.I).strip()\n",
        "    fixes = {\n",
        "        r\"from\\s+sklearn\\.pipeline\\s+import\\s+SimpleImputer\": \"from sklearn.impute import SimpleImputer\",\n",
        "        r\"from\\s+sklearn\\.preprocessing\\s+import\\s+SimpleImputer\": \"from sklearn.impute import SimpleImputer\",\n",
        "        r\"from\\s+sklearn\\.pipeline\\s+import\\s+StandardScaler\": \"from sklearn.preprocessing import StandardScaler\",\n",
        "        r\"from\\s+sklearn\\.preprocessing\\s+import\\s+ColumnTransformer\": \"from sklearn.compose import ColumnTransformer\",\n",
        "        r\"from\\s+sklearn\\.pipeline\\s+import\\s+ColumnTransformer\": \"from sklearn.compose import ColumnTransformer\",\n",
        "    }\n",
        "    for pat,rep in fixes.items(): s = re.sub(pat, rep, s)\n",
        "    if \"SimpleImputer\" in s and \"from sklearn.impute import SimpleImputer\" not in s:\n",
        "        s = \"from sklearn.impute import SimpleImputer\\n\"+s\n",
        "    if \"StandardScaler\" in s and \"from sklearn.preprocessing import StandardScaler\" not in s:\n",
        "        s = \"from sklearn.preprocessing import StandardScaler\\n\"+s\n",
        "    if \"ColumnTransformer\" in s and \"from sklearn.compose import ColumnTransformer\" not in s:\n",
        "        s = \"from sklearn.compose import ColumnTransformer\\n\"+s\n",
        "    if \"train_test_split\" in s and \"from sklearn.model_selection import train_test_split\" not in s:\n",
        "        s = \"from sklearn.model_selection import train_test_split\\n\"+s\n",
        "    if \"joblib\" in s and \"import joblib\" not in s: s = \"import joblib\\n\"+s\n",
        "    return s\n",
        "\n",
        "san = sanitize(code)\n",
        "\n",
        "safe = textwrap.dedent(f\"\"\"\n",
        "import pandas as pd, numpy as np, joblib\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "DATA=Path(\"{DATA}\"); MODEL=Path(\"{MODEL}\"); PREDS=Path(\"{PREDS}\")\n",
        "df=pd.read_csv(DATA); X=df.drop(columns=[\"target\"]); y=df[\"target\"].astype(int)\n",
        "num=X.columns.tolist()\n",
        "pre=ColumnTransformer([(\"num\",Pipeline([(\"imp\",SimpleImputer()),(\"sc\",StandardScaler())]),num)])\n",
        "clf=LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "pipe=Pipeline([(\"pre\",pre),(\"clf\",clf)])\n",
        "Xtr,Xte,ytr,yte=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
        "pipe.fit(Xtr,ytr)\n",
        "proba=pipe.predict_proba(Xte)[:,1]; pred=(proba>=0.5).astype(int)\n",
        "print(\"ROC-AUC:\",round(roc_auc_score(yte,proba),4)); print(\"F1:\",round(f1_score(yte,pred),4))\n",
        "import pandas as pd\n",
        "coef=pd.Series(pipe.named_steps[\"clf\"].coef_.ravel(), index=num).abs().sort_values(ascending=False)\n",
        "print(\"Top coefficients by |magnitude|:\\\\n\", coef.to_string())\n",
        "joblib.dump(pipe,MODEL)\n",
        "pd.DataFrame({{\"y_true\":yte.reset_index(drop=True),\"y_prob\":proba,\"y_pred\":pred}}).to_csv(PREDS,index=False)\n",
        "print(\"Saved:\",MODEL,PREDS)\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "mfL5T5U4L8ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7qOGBxAGRw1",
        "outputId": "4c28f646-05d1-4717-c530-257e2bf9fd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ pip -q install --upgrade pip\n",
            "\n",
            "$ pip -q install mle-agent==0.4.* scikit-learn pandas numpy joblib\n",
            "\n",
            "$ curl -fsSL https://ollama.com/install.sh | sh\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "#=#=#                                                                         \n",
            "##O#-#                                                                        \n",
            "##O=#  #                                                                      \n",
            "#=#=-#  #                                                                     \n",
            "-#O#- #   #                                                                   \n",
            "-=#=#   #   #                                                                 \n",
            "\n",
            "                                                                           0.0%\n",
            "                                                                           0.0%\n",
            "                                                                           0.1%\n",
            "                                                                           0.5%\n",
            "                                                                           0.8%\n",
            "                                                                           1.1%\n",
            "#                                                                          1.4%\n",
            "#                                                                          1.6%\n",
            "#                                                                          1.8%\n",
            "#                                                                          2.0%\n",
            "#                                                                          2.2%\n",
            "#                                                                          2.4%\n",
            "#                                                                          2.6%\n",
            "##                                                                         2.8%\n",
            "##                                                                         3.2%\n",
            "##                                                                         3.4%\n",
            "##                                                                         3.7%\n",
            "##                                                                         4.0%\n",
            "###                                                                        4.4%\n",
            "###                                                                        4.7%\n",
            "###                                                                        5.0%\n",
            "###                                                                        5.3%\n",
            "###                                                                        5.5%\n",
            "####                                                                       5.8%\n",
            "####                                                                       6.0%\n",
            "####                                                                       6.2%\n",
            "####                                                                       6.4%\n",
            "####                                                                       6.6%\n",
            "####                                                                       6.8%\n",
            "#####                                                                      7.0%\n",
            "#####                                                                      7.4%\n",
            "#####                                                                      7.7%\n",
            "#####                                                                      8.0%\n",
            "######                                                                     8.3%\n",
            "######                                                                     8.7%\n",
            "######                                                                     9.0%\n",
            "######                                                                     9.3%\n",
            "######                                                                     9.5%\n",
            "#######                                                                    9.7%\n",
            "#######                                                                   10.1%\n",
            "#######                                                                   10.3%\n",
            "#######                                                                   10.6%\n",
            "#######                                                                   10.8%\n",
            "#######                                                                   11.0%\n",
            "########                                                                  11.2%\n",
            "########                                                                  11.5%\n",
            "########                                                                  11.7%\n",
            "########                                                                  11.9%\n",
            "########                                                                  12.1%\n",
            "########                                                                  12.3%\n",
            "#########                                                                 12.5%\n",
            "#########                                                                 12.8%\n",
            "#########                                                                 13.0%\n",
            "#########                                                                 13.2%\n",
            "#########                                                                 13.3%\n",
            "#########                                                                 13.5%\n",
            "#########                                                                 13.7%\n",
            "##########                                                                13.9%\n",
            "##########                                                                14.1%\n",
            "##########                                                                14.4%\n",
            "##########                                                                14.6%\n",
            "##########                                                                14.8%\n",
            "##########                                                                15.0%\n",
            "##########                                                                15.2%\n",
            "###########                                                               15.4%\n",
            "###########                                                               15.7%\n",
            "###########                                                               15.9%\n",
            "###########                                                               16.2%\n",
            "###########                                                               16.5%\n",
            "############                                                              16.8%\n",
            "############                                                              17.2%\n",
            "############                                                              17.4%\n",
            "############                                                              17.7%\n",
            "############                                                              18.0%\n",
            "#############                                                             18.3%\n",
            "#############                                                             18.7%\n",
            "#############                                                             19.1%\n",
            "##############                                                            19.5%\n",
            "##############                                                            20.0%\n",
            "##############                                                            20.4%\n",
            "###############                                                           20.9%\n",
            "###############                                                           21.5%\n",
            "###############                                                           21.9%\n",
            "################                                                          22.3%\n",
            "################                                                          22.6%\n",
            "################                                                          23.1%\n",
            "#################                                                         23.7%\n",
            "#################                                                         24.3%\n",
            "#################                                                         24.8%\n",
            "##################                                                        25.2%\n",
            "##################                                                        25.9%\n",
            "###################                                                       26.4%\n",
            "###################                                                       27.1%\n",
            "###################                                                       27.5%\n",
            "####################                                                      27.9%\n",
            "####################                                                      28.3%\n",
            "####################                                                      28.7%\n",
            "####################                                                      29.2%\n",
            "#####################                                                     29.6%\n",
            "#####################                                                     30.0%\n",
            "#####################                                                     30.5%\n",
            "######################                                                    30.9%\n",
            "######################                                                    31.3%\n",
            "######################                                                    31.7%\n",
            "#######################                                                   32.2%\n",
            "#######################                                                   32.6%\n",
            "#######################                                                   33.0%\n",
            "########################                                                  33.5%\n",
            "########################                                                  33.9%\n",
            "########################                                                  34.3%\n",
            "#########################                                                 34.8%\n",
            "#########################                                                 35.2%\n",
            "#########################                                                 35.5%\n",
            "#########################                                                 35.8%\n",
            "##########################                                                36.2%\n",
            "##########################                                                36.7%\n",
            "##########################                                                37.1%\n",
            "###########################                                               37.5%\n",
            "###########################                                               38.2%\n",
            "###########################                                               38.8%\n",
            "############################                                              39.0%\n",
            "############################                                              39.6%\n",
            "############################                                              40.2%\n",
            "#############################                                             40.6%\n",
            "#############################                                             41.2%\n",
            "##############################                                            41.8%\n",
            "##############################                                            42.3%\n",
            "##############################                                            42.8%\n",
            "###############################                                           43.3%\n",
            "###############################                                           44.0%\n",
            "################################                                          44.4%\n",
            "################################                                          45.1%\n",
            "################################                                          45.5%\n",
            "################################                                          45.8%\n",
            "#################################                                         46.1%\n",
            "#################################                                         46.4%\n",
            "#################################                                         46.7%\n",
            "##################################                                        47.3%\n",
            "##################################                                        48.0%\n",
            "###################################                                       48.6%\n",
            "###################################                                       49.3%\n",
            "###################################                                       49.9%\n",
            "####################################                                      50.6%\n",
            "####################################                                      51.3%\n",
            "#####################################                                     51.9%\n",
            "#####################################                                     52.6%\n",
            "######################################                                    53.1%\n",
            "######################################                                    53.8%\n",
            "######################################                                    54.1%\n",
            "#######################################                                   54.5%\n",
            "#######################################                                   55.0%\n",
            "#######################################                                   55.4%\n",
            "########################################                                  55.8%\n",
            "########################################                                  56.2%\n",
            "########################################                                  56.7%\n",
            "#########################################                                 57.0%\n",
            "#########################################                                 57.3%\n",
            "#########################################                                 57.6%\n",
            "#########################################                                 58.1%\n",
            "##########################################                                58.5%\n",
            "##########################################                                59.0%\n",
            "##########################################                                59.4%\n",
            "##########################################                                59.7%\n",
            "###########################################                               60.1%\n",
            "###########################################                               60.4%\n",
            "###########################################                               60.9%\n",
            "###########################################                               61.1%\n",
            "############################################                              61.2%\n",
            "############################################                              61.4%\n",
            "############################################                              61.6%\n",
            "############################################                              61.8%\n",
            "############################################                              62.1%\n",
            "############################################                              62.3%\n",
            "############################################                              62.5%\n",
            "#############################################                             62.7%\n",
            "#############################################                             62.9%\n",
            "#############################################                             63.1%\n",
            "#############################################                             63.3%\n",
            "#############################################                             63.6%\n",
            "#############################################                             63.8%\n",
            "##############################################                            64.1%\n",
            "##############################################                            64.4%\n",
            "##############################################                            64.7%\n",
            "##############################################                            65.1%\n",
            "###############################################                           65.4%\n",
            "###############################################                           65.6%\n",
            "###############################################                           65.7%\n",
            "###############################################                           65.8%\n",
            "###############################################                           66.0%\n",
            "###############################################                           66.3%\n",
            "###############################################                           66.5%\n",
            "################################################                          66.7%\n",
            "################################################                          67.0%\n",
            "################################################                          67.3%\n",
            "################################################                          67.6%\n",
            "################################################                          68.0%\n",
            "#################################################                         68.3%\n",
            "#################################################                         68.6%\n",
            "#################################################                         68.9%\n",
            "#################################################                         69.3%\n",
            "##################################################                        69.5%\n",
            "##################################################                        69.7%\n",
            "##################################################                        70.0%\n",
            "##################################################                        70.3%\n",
            "##################################################                        70.7%\n",
            "###################################################                       71.0%\n",
            "###################################################                       71.3%\n",
            "###################################################                       71.6%\n",
            "###################################################                       71.9%\n",
            "####################################################                      72.2%\n",
            "####################################################                      72.6%\n",
            "####################################################                      72.9%\n",
            "####################################################                      73.2%\n",
            "####################################################                      73.5%\n",
            "#####################################################                     73.9%\n",
            "#####################################################                     74.2%\n",
            "#####################################################                     74.5%\n",
            "#####################################################                     74.8%\n",
            "######################################################                    75.1%\n",
            "######################################################                    75.5%\n",
            "######################################################                    75.7%\n",
            "######################################################                    75.9%\n",
            "######################################################                    76.1%\n",
            "#######################################################                   76.4%\n",
            "#######################################################                   76.8%\n",
            "#######################################################                   77.1%\n",
            "#######################################################                   77.4%\n",
            "#######################################################                   77.6%\n",
            "########################################################                  77.9%\n",
            "########################################################                  78.3%\n",
            "########################################################                  78.6%\n",
            "########################################################                  78.9%\n",
            "#########################################################                 79.2%\n",
            "#########################################################                 79.6%\n",
            "#########################################################                 79.9%\n",
            "#########################################################                 80.2%\n",
            "#########################################################                 80.5%\n",
            "##########################################################                80.8%\n",
            "##########################################################                81.2%\n",
            "##########################################################                81.5%\n",
            "##########################################################                81.7%\n",
            "##########################################################                81.9%\n",
            "###########################################################               82.1%\n",
            "###########################################################               82.5%\n",
            "###########################################################               82.8%\n",
            "###########################################################               83.1%\n",
            "############################################################              83.4%\n",
            "############################################################              83.7%\n",
            "############################################################              84.1%\n",
            "############################################################              84.4%\n",
            "############################################################              84.7%\n",
            "#############################################################             85.0%\n",
            "#############################################################             85.4%\n",
            "#############################################################             85.7%\n",
            "#############################################################             86.0%\n",
            "##############################################################            86.3%\n",
            "##############################################################            86.7%\n",
            "##############################################################            87.0%\n",
            "##############################################################            87.3%\n",
            "###############################################################           87.6%\n",
            "###############################################################           87.9%\n",
            "###############################################################           88.2%\n",
            "###############################################################           88.4%\n",
            "###############################################################           88.6%\n",
            "###############################################################           88.8%\n",
            "################################################################          89.1%\n",
            "################################################################          89.5%\n",
            "################################################################          89.7%\n",
            "################################################################          90.0%\n",
            "#################################################################         90.3%\n",
            "#################################################################         90.6%\n",
            "#################################################################         90.9%\n",
            "#################################################################         91.1%\n",
            "#################################################################         91.3%\n",
            "#################################################################         91.5%\n",
            "##################################################################        91.7%\n",
            "##################################################################        91.9%\n",
            "##################################################################        92.1%\n",
            "##################################################################        92.4%\n",
            "##################################################################        92.6%\n",
            "##################################################################        92.7%\n",
            "##################################################################        92.9%\n",
            "##################################################################        93.0%\n",
            "###################################################################       93.2%\n",
            "###################################################################       93.5%\n",
            "###################################################################       93.8%\n",
            "###################################################################       94.1%\n",
            "###################################################################       94.3%\n",
            "####################################################################      94.5%\n",
            "####################################################################      94.7%\n",
            "####################################################################      94.9%\n",
            "####################################################################      95.2%\n",
            "####################################################################      95.4%\n",
            "####################################################################      95.6%\n",
            "####################################################################      95.8%\n",
            "#####################################################################     96.0%\n",
            "#####################################################################     96.2%\n",
            "#####################################################################     96.4%\n",
            "#####################################################################     96.7%\n",
            "#####################################################################     97.0%\n",
            "######################################################################    97.3%\n",
            "######################################################################    97.5%\n",
            "######################################################################    97.8%\n",
            "######################################################################    98.2%\n",
            "######################################################################    98.5%\n",
            "#######################################################################   98.7%\n",
            "#######################################################################   98.9%\n",
            "#######################################################################   99.2%\n",
            "#######################################################################   99.6%\n",
            "#######################################################################   99.9%\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "$ ollama pull llama3.2:1b\n",
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 74701a8c35f6: 100% ▕██████████████████▏ 1.3 GB                         \u001b[K\n",
            "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
            "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
            "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
            "pulling 4f659a1e86d7: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n",
            "\n",
            "$ printf %s \"Return ONE fenced python code block only.\n",
            "Write train.py that reads /content/mle_colab_demo/data.csv; 80/20 split (random_state=42, stratify);\n",
            "Pipeline: SimpleImputer + StandardScaler + LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42);\n",
            "Print ROC-AUC & F1; print sorted coefficient magnitudes; save model to /content/mle_colab_demo/model.joblib and preds to /content/mle_colab_demo/preds.csv;\n",
            "Use only sklearn, pandas, numpy, joblib; no extra text.\" | mle chat\n",
            "[23:02:51] Configuration file not found. Please run 'mle new'       system.py:98\n",
            "           first.                                                               \n",
            "\n",
            "$ printf %s \"Return ONE fenced python code block only.\n",
            "Write train.py that reads /content/mle_colab_demo/data.csv; 80/20 split (random_state=42, stratify);\n",
            "Pipeline: SimpleImputer + StandardScaler + LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42);\n",
            "Print ROC-AUC & F1; print sorted coefficient magnitudes; save model to /content/mle_colab_demo/model.joblib and preds to /content/mle_colab_demo/preds.csv;\n",
            "Use only sklearn, pandas, numpy, joblib; no extra text.\" | ollama run llama3.2:1b\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25himport\u001b[?25l\u001b[?25h pandas\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h pd\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h sklearn\u001b[?25l\u001b[?25h.im\u001b[?25l\u001b[?25hpute\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h Simple\u001b[?25l\u001b[?25hIm\u001b[?25l\u001b[?25hputer\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h sklearn\u001b[?25l\u001b[?25h.preprocessing\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h Standard\u001b[?25l\u001b[?25hScaler\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h sklearn\u001b[?25l\u001b[?25h.linear\u001b[?25l\u001b[?25h_model\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h Logistic\u001b[?25l\u001b[?25hRegression\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h sklearn\u001b[?25l\u001b[?25h.metrics\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h roc\u001b[?25l\u001b[?25h_auc\u001b[?25l\u001b[?25h_score\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h f\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h_score\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h sklearn\u001b[?25l\u001b[?25h.pipeline\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h Pipeline\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25himport\u001b[?25l\u001b[?25h numpy\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25himport\u001b[?25l\u001b[?25h job\u001b[?25l\u001b[?25hlib\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25hwith\u001b[?25l\u001b[?25h pd\u001b[?25l\u001b[?25h.option\u001b[?25l\u001b[?25h_context\u001b[?25l\u001b[?25h('\u001b[?25l\u001b[?25hdisplay\u001b[?25l\u001b[?25h.max\u001b[?25l\u001b[?25h_rows\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h None\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hdisplay\u001b[?25l\u001b[?25h.max\u001b[?25l\u001b[?25h_columns\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h None\u001b[?25l\u001b[?25h):\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h df\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h pd\u001b[?25l\u001b[?25h.read\u001b[?25l\u001b[?25h_csv\u001b[?25l\u001b[?25h('/\u001b[?25l\u001b[?25hcontent\u001b[?25l\u001b[?25h/m\u001b[?25l\u001b[?25hle\u001b[?25l\u001b[?25h_col\u001b[?25l\u001b[?25hab\u001b[?25l\u001b[?25h_demo\u001b[?25l\u001b[?25h/data\u001b[?25l\u001b[?25h.csv\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h use\u001b[?25l\u001b[?25hcols\u001b[?25l\u001b[?25h=[\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h],\u001b[?25l\u001b[?25h na\u001b[?25l\u001b[?25h_values\u001b[?25l\u001b[?25h=['\u001b[?25l\u001b[?25hNA\u001b[?25l\u001b[?25h'])\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h X\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h X\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h df\u001b[?25l\u001b[?25h[\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h].\u001b[?25l\u001b[?25hstr\u001b[?25l\u001b[?25h.split\u001b[?25l\u001b[?25h(',',\u001b[?25l\u001b[?25h n\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h expand\u001b[?25l\u001b[?25h=True\u001b[?25l\u001b[?25h).\u001b[?25l\u001b[?25hastype\u001b[?25l\u001b[?25h(int\u001b[?25l\u001b[?25h).\u001b[?25l\u001b[?25hvalues\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h df\u001b[?25l\u001b[?25h[\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h].\u001b[?25l\u001b[?25hstr\u001b[?25l\u001b[?25h.split\u001b[?25l\u001b[?25h(',',\u001b[?25l\u001b[?25h n\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h expand\u001b[?25l\u001b[?25h=True\u001b[?25l\u001b[?25h).\u001b[?25l\u001b[?25hvalues\u001b[?25l\u001b[?25h.astype\u001b[?25l\u001b[?25h(int\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h df\u001b[?25l\u001b[?25h[\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h].\u001b[?25l\u001b[?25hvalues\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h df\u001b[?25l\u001b[?25h[\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h].\u001b[?25l\u001b[?25hvalues\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h {'\u001b[?25l\u001b[?25hX\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h [\u001b[?25l\u001b[?25hnp\u001b[?25l\u001b[?25h.array\u001b[?25l\u001b[?25h(X\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.array\u001b[?25l\u001b[?25h(X\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h)],\u001b[?25l\u001b[?25h \n",
            "\u001b[?25l\u001b[?25h       \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h [\u001b[?25l\u001b[?25hnp\u001b[?25l\u001b[?25h.array\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_train\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.array\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h)]\u001b[?25l\u001b[?25h}\n",
            "\n",
            "\u001b[?25l\u001b[?25hpipeline\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h Pipeline\u001b[?25l\u001b[?25h([\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h ('\u001b[?25l\u001b[?25him\u001b[?25l\u001b[?25hputer\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h Simple\u001b[?25l\u001b[?25hIm\u001b[?25l\u001b[?25hputer\u001b[?25l\u001b[?25h(strategy\u001b[?25l\u001b[?25h='\u001b[?25l\u001b[?25hmedian\u001b[?25l\u001b[?25h')),\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h ('\u001b[?25l\u001b[?25hsc\u001b[?25l\u001b[?25haler\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h Standard\u001b[?25l\u001b[?25hScaler\u001b[?25l\u001b[?25h()),\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h ('\u001b[?25l\u001b[?25hclassifier\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h Logistic\u001b[?25l\u001b[?25hRegression\u001b[?25l\u001b[?25h(class\u001b[?25l\u001b[?25h_weight\u001b[?25l\u001b[?25h='\u001b[?25l\u001b[?25hbalanced\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h max\u001b[?25l\u001b[?25h_iter\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h100\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h random\u001b[?25l\u001b[?25h_state\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h42\u001b[?25l\u001b[?25h))\n",
            "\u001b[?25l\u001b[?25h])\n",
            "\n",
            "\u001b[?25l\u001b[?25hpipeline\u001b[?25l\u001b[?25h.fit\u001b[?25l\u001b[?25h(data\u001b[?25l\u001b[?25h['\u001b[?25l\u001b[?25hX\u001b[?25l\u001b[?25h'],\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h['\u001b[?25l\u001b[?25hy\u001b[?25l\u001b[?25h'])\n",
            "\n",
            "\u001b[?25l\u001b[?25hprint\u001b[?25l\u001b[?25h(\"\u001b[?25l\u001b[?25hROC\u001b[?25l\u001b[?25h-A\u001b[?25l\u001b[?25hUC\u001b[?25l\u001b[?25h:\",\u001b[?25l\u001b[?25h roc\u001b[?25l\u001b[?25h_auc\u001b[?25l\u001b[?25h_score\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h pipeline\u001b[?25l\u001b[?25h.predict\u001b[?25l\u001b[?25h_proba\u001b[?25l\u001b[?25h(X\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h[:,\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h]))\n",
            "\u001b[?25l\u001b[?25hprint\u001b[?25l\u001b[?25h(\"\u001b[?25l\u001b[?25hF\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h:\",\u001b[?25l\u001b[?25h f\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h_score\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h np\u001b[?25l\u001b[?25h.array\u001b[?25l\u001b[?25h([\u001b[?25l\u001b[?25hint\u001b[?25l\u001b[?25h(x\u001b[?25l\u001b[?25h[\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h]\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h x\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h pipeline\u001b[?25l\u001b[?25h.predict\u001b[?25l\u001b[?25h_proba\u001b[?25l\u001b[?25h(X\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h))\u001b[?25l\u001b[?25h])\u001b[?25l\u001b[?25h /\u001b[?25l\u001b[?25h len\u001b[?25l\u001b[?25h(y\u001b[?25l\u001b[?25h_test\u001b[?25l\u001b[?25h),\u001b[?25l\u001b[?25h average\u001b[?25l\u001b[?25h='\u001b[?25l\u001b[?25hmacro\u001b[?25l\u001b[?25h'))\n",
            "\u001b[?25l\u001b[?25hprint\u001b[?25l\u001b[?25h(sorted\u001b[?25l\u001b[?25h(np\u001b[?25l\u001b[?25h.abs\u001b[?25l\u001b[?25h(p\u001b[?25l\u001b[?25hipeline\u001b[?25l\u001b[?25h.co\u001b[?25l\u001b[?25hef\u001b[?25l\u001b[?25h_[\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h]))\u001b[?25l\u001b[?25h[:\u001b[?25l\u001b[?25h10\u001b[?25l\u001b[?25h])\n",
            "\u001b[?25l\u001b[?25hjob\u001b[?25l\u001b[?25hlib\u001b[?25l\u001b[?25h.dump\u001b[?25l\u001b[?25h(p\u001b[?25l\u001b[?25hipeline\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h '/\u001b[?25l\u001b[?25hcontent\u001b[?25l\u001b[?25h/m\u001b[?25l\u001b[?25hle\u001b[?25l\u001b[?25h_col\u001b[?25l\u001b[?25hab\u001b[?25l\u001b[?25h_demo\u001b[?25l\u001b[?25h/model\u001b[?25l\u001b[?25h.job\u001b[?25l\u001b[?25hlib\u001b[?25l\u001b[?25h')\n",
            "\u001b[?25l\u001b[?25hjob\u001b[?25l\u001b[?25hlib\u001b[?25l\u001b[?25h.dump\u001b[?25l\u001b[?25h(df\u001b[?25l\u001b[?25h.iloc\u001b[?25l\u001b[?25h[:,\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h],\u001b[?25l\u001b[?25h '/\u001b[?25l\u001b[?25hcontent\u001b[?25l\u001b[?25h/m\u001b[?25l\u001b[?25hle\u001b[?25l\u001b[?25h_col\u001b[?25l\u001b[?25hab\u001b[?25l\u001b[?25h_demo\u001b[?25l\u001b[?25h/p\u001b[?25l\u001b[?25hreds\u001b[?25l\u001b[?25h.csv\u001b[?25l\u001b[?25h')\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h\n",
            "\n",
            "=== Using train.py (first 800 chars) ===\n",
            " import pandas as pd, numpy as np, joblib\n",
            "from pathlib import Path\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.impute import SimpleImputer\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.metrics import roc_auc_score, f1_score\n",
            "from sklearn.compose import ColumnTransformer\n",
            "\n",
            "DATA=Path(\"/content/mle_colab_demo/data.csv\"); MODEL=Path(\"/content/mle_colab_demo/model.joblib\"); PREDS=Path(\"/content/mle_colab_demo/preds.csv\")\n",
            "df=pd.read_csv(DATA); X=df.drop(columns=[\"target\"]); y=df[\"target\"].astype(int)\n",
            "num=X.columns.tolist()\n",
            "pre=ColumnTransformer([(\"num\",Pipeline([(\"imp\",SimpleImputer()),(\"sc\",StandardScaler())]),num)])\n",
            "clf=LogisticRegression(class_weight='balanced', max_ite \n",
            "...\n",
            "$ python /content/mle_colab_demo/train.py\n",
            "ROC-AUC: 0.8569\n",
            "F1: 0.6769\n",
            "Top coefficients by |magnitude|:\n",
            " f4    1.766433\n",
            "f1    1.234575\n",
            "f2    0.790912\n",
            "f3    0.110197\n",
            "Saved: /content/mle_colab_demo/model.joblib /content/mle_colab_demo/preds.csv\n",
            "\n",
            "\n",
            "Artifacts: ['/content/mle_colab_demo/model.joblib', '/content/mle_colab_demo/agent_train_raw.py', '/content/mle_colab_demo/proj', '/content/mle_colab_demo/preds.csv', '/content/mle_colab_demo/data.csv', '/content/mle_colab_demo/train.py', '/content/mle_colab_demo/train_safe.py']\n",
            "✅ Done — outputs in /content/mle_colab_demo\n"
          ]
        }
      ],
      "source": [
        "chosen = san if (\"import \" in san and \"sklearn\" in san and \"read_csv\" in san) else safe\n",
        "Path(SAFE).write_text(safe, encoding=\"utf-8\")\n",
        "Path(FINAL).write_text(chosen, encoding=\"utf-8\")\n",
        "print(\"\\n=== Using train.py (first 800 chars) ===\\n\", chosen[:800], \"\\n...\")\n",
        "\n",
        "sh(f\"python {FINAL}\")\n",
        "print(\"\\nArtifacts:\", [str(p) for p in WORK.glob('*')])\n",
        "print(\"✅ Done — outputs in\", WORK)"
      ]
    }
  ]
}