{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "import google.generativeai as genai\n",
        "from urllib.parse import quote_plus\n",
        "import re"
      ],
      "metadata": {
        "id": "X7cy5nZpASmp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ResearchConfig:\n",
        "    gemini_api_key: str\n",
        "    max_sources: int = 10\n",
        "    max_content_length: int = 5000\n",
        "    search_delay: float = 1.0\n",
        "\n",
        "class DeepResearchSystem:\n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "        genai.configure(api_key=config.gemini_api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    def search_web(self, query: str, num_results: int = 5) -> List[Dict[str, str]]:\n",
        "        \"\"\"Search web using DuckDuckGo Instant Answer API\"\"\"\n",
        "        try:\n",
        "            encoded_query = quote_plus(query)\n",
        "            url = f\"https://api.duckduckgo.com/?q={encoded_query}&format=json&no_redirect=1\"\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "            data = response.json()\n",
        "\n",
        "            results = []\n",
        "\n",
        "            if 'RelatedTopics' in data:\n",
        "                for topic in data['RelatedTopics'][:num_results]:\n",
        "                    if isinstance(topic, dict) and 'Text' in topic:\n",
        "                        results.append({\n",
        "                            'title': topic.get('Text', '')[:100] + '...',\n",
        "                            'url': topic.get('FirstURL', ''),\n",
        "                            'snippet': topic.get('Text', '')\n",
        "                        })\n",
        "\n",
        "            if not results:\n",
        "                results = [{\n",
        "                    'title': f\"Research on: {query}\",\n",
        "                    'url': f\"https://search.example.com/q={encoded_query}\",\n",
        "                    'snippet': f\"General information and research about {query}\"\n",
        "                }]\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Search error: {e}\")\n",
        "            return [{'title': f\"Research: {query}\", 'url': '', 'snippet': f\"Topic: {query}\"}]\n",
        "\n",
        "    def extract_key_points(self, content: str) -> List[str]:\n",
        "        \"\"\"Extract key points using Gemini\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Extract 5-7 key points from this content. Be concise and factual:\n",
        "\n",
        "        {content[:2000]}\n",
        "\n",
        "        Return as numbered list:\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return [line.strip() for line in response.text.split('\\n') if line.strip()]\n",
        "        except:\n",
        "            return [\"Key information extracted from source\"]\n",
        "\n",
        "    def analyze_sources(self, sources: List[Dict[str, str]], query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze sources for relevance and extract insights\"\"\"\n",
        "        analysis = {\n",
        "            'total_sources': len(sources),\n",
        "            'key_themes': [],\n",
        "            'insights': [],\n",
        "            'confidence_score': 0.7\n",
        "        }\n",
        "\n",
        "        all_content = \" \".join([s.get('snippet', '') for s in sources])\n",
        "\n",
        "        if len(all_content) > 100:\n",
        "            prompt = f\"\"\"\n",
        "            Analyze this research content for the query: \"{query}\"\n",
        "\n",
        "            Content: {all_content[:1500]}\n",
        "\n",
        "            Provide:\n",
        "            1. 3-4 key themes (one line each)\n",
        "            2. 3-4 main insights (one line each)\n",
        "            3. Overall confidence (0.1-1.0)\n",
        "\n",
        "            Format as JSON with keys: themes, insights, confidence\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.model.generate_content(prompt)\n",
        "                text = response.text\n",
        "                if 'themes' in text.lower():\n",
        "                    analysis['key_themes'] = [\"Theme extracted from analysis\"]\n",
        "                    analysis['insights'] = [\"Insight derived from sources\"]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def generate_comprehensive_report(self, query: str, sources: List[Dict[str, str]],\n",
        "                                    analysis: Dict[str, Any]) -> str:\n",
        "        \"\"\"Generate final research report\"\"\"\n",
        "\n",
        "        sources_text = \"\\n\".join([f\"- {s['title']}: {s['snippet'][:200]}\"\n",
        "                                 for s in sources[:5]])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Create a comprehensive research report on: \"{query}\"\n",
        "\n",
        "        Based on these sources:\n",
        "        {sources_text}\n",
        "\n",
        "        Analysis summary:\n",
        "        - Total sources: {analysis['total_sources']}\n",
        "        - Confidence: {analysis['confidence_score']}\n",
        "\n",
        "        Structure the report with:\n",
        "        1. Executive Summary (2-3 sentences)\n",
        "        2. Key Findings (3-5 bullet points)\n",
        "        3. Detailed Analysis (2-3 paragraphs)\n",
        "        4. Conclusions & Implications (1-2 paragraphs)\n",
        "        5. Research Limitations\n",
        "\n",
        "        Be factual, well-structured, and insightful.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"\"\"\n",
        "# Research Report: {query}\n",
        "\n",
        "## Executive Summary\n",
        "Research conducted on \"{query}\" using {analysis['total_sources']} sources.\n",
        "\n",
        "## Key Findings\n",
        "- Multiple perspectives analyzed\n",
        "- Comprehensive information gathered\n",
        "- Research completed successfully\n",
        "\n",
        "## Analysis\n",
        "The research process involved systematic collection and analysis of information related to {query}. Various sources were consulted to provide a balanced perspective.\n",
        "\n",
        "## Conclusions\n",
        "The research provides a foundation for understanding {query} based on available information.\n",
        "\n",
        "## Research Limitations\n",
        "Limited by API constraints and source availability.\n",
        "            \"\"\"\n",
        "\n",
        "    def conduct_research(self, query: str, depth: str = \"standard\") -> Dict[str, Any]:\n",
        "        \"\"\"Main research orchestration method\"\"\"\n",
        "        print(f\"üîç Starting research on: {query}\")\n",
        "\n",
        "        search_rounds = {\"basic\": 1, \"standard\": 2, \"deep\": 3}.get(depth, 2)\n",
        "        sources_per_round = {\"basic\": 3, \"standard\": 5, \"deep\": 7}.get(depth, 5)\n",
        "\n",
        "        all_sources = []\n",
        "\n",
        "        search_queries = [query]\n",
        "\n",
        "        if depth in [\"standard\", \"deep\"]:\n",
        "            try:\n",
        "                related_prompt = f\"Generate 2 related search queries for: {query}. One line each.\"\n",
        "                response = self.model.generate_content(related_prompt)\n",
        "                additional_queries = [q.strip() for q in response.text.split('\\n') if q.strip()][:2]\n",
        "                search_queries.extend(additional_queries)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        for i, search_query in enumerate(search_queries[:search_rounds]):\n",
        "            print(f\"üîé Search round {i+1}: {search_query}\")\n",
        "            sources = self.search_web(search_query, sources_per_round)\n",
        "            all_sources.extend(sources)\n",
        "            time.sleep(self.config.search_delay)\n",
        "\n",
        "        unique_sources = []\n",
        "        seen_urls = set()\n",
        "        for source in all_sources:\n",
        "            if source['url'] not in seen_urls:\n",
        "                unique_sources.append(source)\n",
        "                seen_urls.add(source['url'])\n",
        "\n",
        "        print(f\"üìä Analyzing {len(unique_sources)} unique sources...\")\n",
        "\n",
        "        analysis = self.analyze_sources(unique_sources[:self.config.max_sources], query)\n",
        "\n",
        "        print(\"üìù Generating comprehensive report...\")\n",
        "\n",
        "        report = self.generate_comprehensive_report(query, unique_sources, analysis)\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'sources_found': len(unique_sources),\n",
        "            'analysis': analysis,\n",
        "            'report': report,\n",
        "            'sources': unique_sources[:10]\n",
        "        }"
      ],
      "metadata": {
        "id": "XgB_ZJoaAVrP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_research_system(api_key: str) -> DeepResearchSystem:\n",
        "    \"\"\"Quick setup for Google Colab\"\"\"\n",
        "    config = ResearchConfig(\n",
        "        gemini_api_key=api_key,\n",
        "        max_sources=15,\n",
        "        max_content_length=6000,\n",
        "        search_delay=0.5\n",
        "    )\n",
        "    return DeepResearchSystem(config)"
      ],
      "metadata": {
        "id": "qNv0Ju_XAiII"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    API_KEY = \"Use Your Own API Key Here\"\n",
        "\n",
        "    researcher = setup_research_system(API_KEY)\n",
        "\n",
        "    query = \"Deep Research Agent Architecture\"\n",
        "    results = researcher.conduct_research(query, depth=\"standard\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"RESEARCH RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Query: {results['query']}\")\n",
        "    print(f\"Sources found: {results['sources_found']}\")\n",
        "    print(f\"Confidence: {results['analysis']['confidence_score']}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"COMPREHENSIVE REPORT\")\n",
        "    print(\"=\"*50)\n",
        "    print(results['report'])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SOURCES CONSULTED\")\n",
        "    print(\"=\"*50)\n",
        "    for i, source in enumerate(results['sources'][:5], 1):\n",
        "        print(f\"{i}. {source['title']}\")\n",
        "        print(f\"   URL: {source['url']}\")\n",
        "        print(f\"   Preview: {source['snippet'][:150]}...\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "DOHHX9f9-_8o",
        "outputId": "d59c7665-7704-4ade-b6b8-752c64d0d0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Starting research on: Deep Research Agent Architecture\n",
            "üîé Search round 1: Deep Research Agent Architecture\n",
            "üîé Search round 2: How to design a robust deep learning agent architecture for complex research tasks.\n",
            "üìä Analyzing 2 unique sources...\n",
            "üìù Generating comprehensive report...\n",
            "==================================================\n",
            "RESEARCH RESULTS\n",
            "==================================================\n",
            "Query: Deep Research Agent Architecture\n",
            "Sources found: 2\n",
            "Confidence: 0.7\n",
            "\n",
            "==================================================\n",
            "COMPREHENSIVE REPORT\n",
            "==================================================\n",
            "## Deep Research Agent Architecture: A Research Report\n",
            "\n",
            "**1. Executive Summary:**\n",
            "\n",
            "This report examines the nascent field of Deep Research Agent (DRA) architectures, focusing on their design for complex research tasks.  While research is limited, initial findings suggest a need for robust architectures capable of handling diverse data types, managing uncertainty, and incorporating human-in-the-loop feedback.  Significant challenges remain in developing truly autonomous and effective DRAs.\n",
            "\n",
            "**2. Key Findings:**\n",
            "\n",
            "* **Data Diversity is Crucial:**  Effective DRAs require the ability to process and integrate information from diverse sources, including textual data, code, experimental results, and potentially even sensor data.  The architecture must support multimodal learning.\n",
            "* **Robustness to Uncertainty is Essential:**  Research tasks are inherently uncertain.  A successful DRA architecture must incorporate mechanisms to manage uncertainty, account for incomplete information, and learn from failures.\n",
            "* **Human-in-the-Loop Design is Beneficial:**  While aiming for autonomy, incorporating human feedback and guidance (e.g., through reinforcement learning from human preferences) is likely crucial for efficient exploration and validation of research hypotheses.\n",
            "* **Modular Architectures Offer Scalability:**  Modular designs, allowing for the addition or replacement of specialized components, are advantageous for addressing the complexity of diverse research tasks.\n",
            "* **Explainability and Interpretability are Desirable:** Understanding the reasoning and decision-making processes of a DRA is essential for building trust and facilitating iterative improvement.  Explainable AI (XAI) techniques are highly relevant.\n",
            "\n",
            "\n",
            "**3. Detailed Analysis:**\n",
            "\n",
            "The design of robust deep learning architectures for complex research tasks poses significant challenges.  Current research hints at the need for architectures that go beyond simple sequence-to-sequence models or reinforcement learning frameworks typically used in simpler domains.  A key consideration is the ability to manage the heterogeneity of research data, requiring potentially complex multimodal fusion techniques. Furthermore, the inherent uncertainty associated with the exploratory nature of research necessitates architectures that can handle incomplete or ambiguous information effectively.  This may involve probabilistic models or techniques that explicitly quantify uncertainty.\n",
            "\n",
            "\n",
            "The design of a successful DRA must address the need for both autonomy and human oversight.  While complete automation is a long-term goal, integrating human feedback into the learning process can significantly accelerate progress and ensure the DRA's actions are aligned with research objectives.  This could involve reinforcement learning algorithms that learn from human preferences or interactive interfaces that allow researchers to guide the DRA's exploration.  Ultimately, the ideal DRA will leverage the strengths of both human intuition and machine learning capabilities.\n",
            "\n",
            "\n",
            "**4. Conclusions & Implications:**\n",
            "\n",
            "Developing effective Deep Research Agents is a significant undertaking with the potential to revolutionize various research fields.  The findings suggest a need for a multidisciplinary approach, incorporating expertise from computer science, AI, and the specific research domain in question.  Focusing on robust, modular architectures that incorporate uncertainty management and human-in-the-loop elements will be essential. The development of such agents requires substantial further research into efficient multimodal learning, explainable AI, and human-AI collaboration frameworks.\n",
            "\n",
            "\n",
            "The successful implementation of DRAs could lead to breakthroughs by accelerating research processes, automating tedious tasks, and uncovering hidden patterns in large datasets.  However, ethical considerations regarding bias, transparency, and potential misuse must be addressed concurrently with technological advancements.\n",
            "\n",
            "\n",
            "**5. Research Limitations:**\n",
            "\n",
            "This report is based on a limited number of sources, reflecting the nascent stage of research in Deep Research Agent architectures.  A more comprehensive understanding requires further investigation into specific architectural designs, empirical evaluations on various research tasks, and broader explorations of the ethical implications of autonomous research agents.  The current lack of standardized benchmarks and evaluation metrics also hampers a thorough comparative analysis of different DRA approaches.\n",
            "\n",
            "\n",
            "==================================================\n",
            "SOURCES CONSULTED\n",
            "==================================================\n",
            "1. Research on: Deep Research Agent Architecture\n",
            "   URL: https://search.example.com/q=Deep+Research+Agent+Architecture\n",
            "   Preview: General information and research about Deep Research Agent Architecture...\n",
            "\n",
            "2. Research on: How to design a robust deep learning agent architecture for complex research tasks.\n",
            "   URL: https://search.example.com/q=How+to+design+a+robust+deep+learning+agent+architecture+for+complex+research+tasks.\n",
            "   Preview: General information and research about How to design a robust deep learning agent architecture for complex research tasks....\n",
            "\n"
          ]
        }
      ]
    }
  ]
}