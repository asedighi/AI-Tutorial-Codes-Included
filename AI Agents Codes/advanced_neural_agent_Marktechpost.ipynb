{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "PWF3m-uLGBM-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedNeuralAgent:\n",
        "    def __init__(self, input_size, hidden_layers=[64, 32], output_size=1, learning_rate=0.001):\n",
        "        \"\"\"Advanced AI Agent with stable training and decision making capabilities\"\"\"\n",
        "        self.lr = learning_rate\n",
        "        self.initial_lr = learning_rate\n",
        "        self.layers = []\n",
        "        self.memory = []\n",
        "        self.performance_history = []\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            fan_in, fan_out = layer_sizes[i], layer_sizes[i+1]\n",
        "            limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "\n",
        "            layer = {\n",
        "                'weights': np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i+1])),\n",
        "                'bias': np.zeros((1, layer_sizes[i+1])),\n",
        "                'momentum_w': np.zeros((layer_sizes[i], layer_sizes[i+1])),\n",
        "                'momentum_b': np.zeros((1, layer_sizes[i+1]))\n",
        "            }\n",
        "            self.layers.append(layer)\n",
        "\n",
        "    def activation(self, x, func='relu'):\n",
        "        \"\"\"Stable activation functions with clipping\"\"\"\n",
        "        x = np.clip(x, -50, 50)\n",
        "\n",
        "        if func == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        elif func == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif func == 'tanh':\n",
        "            return np.tanh(x)\n",
        "        elif func == 'leaky_relu':\n",
        "            return np.where(x > 0, x, x * 0.01)\n",
        "        elif func == 'linear':\n",
        "            return x\n",
        "\n",
        "    def activation_derivative(self, x, func='relu'):\n",
        "        \"\"\"Stable derivatives\"\"\"\n",
        "        x = np.clip(x, -50, 50)\n",
        "\n",
        "        if func == 'relu':\n",
        "            return (x > 0).astype(float)\n",
        "        elif func == 'sigmoid':\n",
        "            s = self.activation(x, 'sigmoid')\n",
        "            return s * (1 - s)\n",
        "        elif func == 'tanh':\n",
        "            return 1 - np.tanh(x)**2\n",
        "        elif func == 'leaky_relu':\n",
        "            return np.where(x > 0, 1, 0.01)\n",
        "        elif func == 'linear':\n",
        "            return np.ones_like(x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass with gradient clipping\"\"\"\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        current_input = X\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            z = np.dot(current_input, layer['weights']) + layer['bias']\n",
        "            z = np.clip(z, -50, 50)\n",
        "            self.z_values.append(z)\n",
        "\n",
        "            if i < len(self.layers) - 1:\n",
        "                a = self.activation(z, 'leaky_relu')\n",
        "            else:\n",
        "                a = self.activation(z, 'linear')\n",
        "\n",
        "            self.activations.append(a)\n",
        "            current_input = a\n",
        "\n",
        "        return current_input\n",
        "\n",
        "    def clip_gradients(self, gradients, max_norm=1.0):\n",
        "        \"\"\"Gradient clipping to prevent explosion\"\"\"\n",
        "        grad_norm = np.linalg.norm(gradients)\n",
        "        if grad_norm > max_norm:\n",
        "            gradients = gradients * (max_norm / (grad_norm + self.epsilon))\n",
        "        return gradients\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        \"\"\"Stable backpropagation with gradient clipping\"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        dz = (output - y.reshape(-1, 1)) / m\n",
        "        dz = np.clip(dz, -10, 10)\n",
        "\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            layer = self.layers[i]\n",
        "\n",
        "            dw = np.dot(self.activations[i].T, dz)\n",
        "            db = np.sum(dz, axis=0, keepdims=True)\n",
        "\n",
        "            dw = self.clip_gradients(dw, max_norm=1.0)\n",
        "            db = self.clip_gradients(db, max_norm=1.0)\n",
        "\n",
        "            momentum = 0.9\n",
        "            layer['momentum_w'] = momentum * layer['momentum_w'] + (1 - momentum) * dw\n",
        "            layer['momentum_b'] = momentum * layer['momentum_b'] + (1 - momentum) * db\n",
        "\n",
        "            weight_decay = 0.0001\n",
        "            layer['weights'] -= self.lr * (layer['momentum_w'] + weight_decay * layer['weights'])\n",
        "            layer['bias'] -= self.lr * layer['momentum_b']\n",
        "\n",
        "            if i > 0:\n",
        "                activation_func = 'leaky_relu' if i > 1 else 'leaky_relu'\n",
        "                dz = np.dot(dz, layer['weights'].T) * self.activation_derivative(\n",
        "                    self.z_values[i-1], activation_func)\n",
        "                dz = np.clip(dz, -10, 10)\n",
        "\n",
        "    def adapt_learning_rate(self, epoch, performance_history):\n",
        "        \"\"\"Adaptive learning rate with performance-based adjustment\"\"\"\n",
        "        if epoch > 10:\n",
        "            recent_performance = performance_history[-10:]\n",
        "            if len(recent_performance) >= 5:\n",
        "                if recent_performance[-1] >= recent_performance[-5]:\n",
        "                    self.lr = max(self.lr * 0.95, self.initial_lr * 0.01)\n",
        "                elif recent_performance[-1] < recent_performance[-5] * 0.98:\n",
        "                    self.lr = min(self.lr * 1.02, self.initial_lr * 2)\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred):\n",
        "        \"\"\"Stable loss calculation\"\"\"\n",
        "        y_true = y_true.reshape(-1, 1)\n",
        "        y_pred = np.clip(y_pred, -1e6, 1e6)\n",
        "\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "        mae = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "        if not np.isfinite(mse):\n",
        "            mse = 1e6\n",
        "        if not np.isfinite(mae):\n",
        "            mae = 1e6\n",
        "\n",
        "        return mse, mae\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state):\n",
        "        \"\"\"Experience replay for RL aspects\"\"\"\n",
        "        experience = {\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'timestamp': len(self.memory)\n",
        "        }\n",
        "        self.memory.append(experience)\n",
        "\n",
        "        if len(self.memory) > 1000:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def make_decision(self, X, exploration_rate=0.1):\n",
        "        \"\"\"Stable decision making\"\"\"\n",
        "        prediction = self.forward(X)\n",
        "\n",
        "        if np.random.random() < exploration_rate:\n",
        "            noise_scale = np.std(prediction) * 0.1 if np.std(prediction) > 0 else 0.1\n",
        "            noise = np.random.normal(0, noise_scale, prediction.shape)\n",
        "            prediction += noise\n",
        "\n",
        "        return np.clip(prediction, -1e6, 1e6)\n",
        "\n",
        "    def reset_if_unstable(self):\n",
        "        \"\"\"Reset network if training becomes unstable\"\"\"\n",
        "        print(\"🔄 Resetting network due to instability...\")\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            fan_in, fan_out = layer['weights'].shape\n",
        "            limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "            layer['weights'] = np.random.uniform(-limit, limit, (fan_in, fan_out))\n",
        "            layer['bias'] = np.zeros((1, fan_out))\n",
        "            layer['momentum_w'] = np.zeros((fan_in, fan_out))\n",
        "            layer['momentum_b'] = np.zeros((1, fan_out))\n",
        "        self.lr = self.initial_lr\n",
        "\n",
        "    def train(self, X, y, epochs=500, batch_size=32, validation_split=0.2, verbose=True):\n",
        "        \"\"\"Robust training with stability checks\"\"\"\n",
        "        y_mean, y_std = np.mean(y), np.std(y)\n",
        "        y_normalized = (y - y_mean) / (y_std + self.epsilon)\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y_normalized, test_size=validation_split, random_state=42)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience = 30\n",
        "        patience_counter = 0\n",
        "\n",
        "        train_losses, val_losses = [], []\n",
        "        reset_count = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            if epoch > 0 and (not np.isfinite(train_losses[-1]) or train_losses[-1] > 1e6):\n",
        "                if reset_count < 2:\n",
        "                    self.reset_if_unstable()\n",
        "                    reset_count += 1\n",
        "                    continue\n",
        "                else:\n",
        "                    print(\"🚫 Training unstable, stopping...\")\n",
        "                    break\n",
        "\n",
        "            indices = np.random.permutation(len(X_train))\n",
        "            X_train_shuffled = X_train[indices]\n",
        "            y_train_shuffled = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            batches = 0\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train_shuffled[i:i+batch_size]\n",
        "                batch_y = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "                if len(batch_X) == 0:\n",
        "                    continue\n",
        "\n",
        "                output = self.forward(batch_X)\n",
        "                self.backward(batch_X, batch_y, output)\n",
        "\n",
        "                loss, _ = self.calculate_loss(batch_y, output)\n",
        "                epoch_loss += loss\n",
        "                batches += 1\n",
        "\n",
        "            avg_train_loss = epoch_loss / max(batches, 1)\n",
        "\n",
        "            val_output = self.forward(X_val)\n",
        "            val_loss, val_mae = self.calculate_loss(y_val, val_output)\n",
        "\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            self.performance_history.append(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                if verbose:\n",
        "                    print(f\"✋ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            if epoch > 0:\n",
        "                self.adapt_learning_rate(epoch, self.performance_history)\n",
        "\n",
        "            if verbose and (epoch % 50 == 0 or epoch < 10):\n",
        "                print(f\"Epoch {epoch:3d}: Train Loss = {avg_train_loss:.4f}, \"\n",
        "                      f\"Val Loss = {val_loss:.4f}, LR = {self.lr:.6f}\")\n",
        "\n",
        "        self.y_mean, self.y_std = y_mean, y_std\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions with denormalization\"\"\"\n",
        "        normalized_pred = self.forward(X)\n",
        "        if hasattr(self, 'y_mean') and hasattr(self, 'y_std'):\n",
        "            return normalized_pred * self.y_std + self.y_mean\n",
        "        return normalized_pred\n",
        "\n",
        "    def evaluate_performance(self, X, y):\n",
        "        \"\"\"Comprehensive performance evaluation\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        mse, mae = self.calculate_loss(y, predictions)\n",
        "\n",
        "        y_mean = np.mean(y)\n",
        "        ss_tot = np.sum((y - y_mean) ** 2)\n",
        "        ss_res = np.sum((y.reshape(-1, 1) - predictions) ** 2)\n",
        "        r2 = 1 - (ss_res / (ss_tot + self.epsilon))\n",
        "\n",
        "        return {\n",
        "            'mse': float(mse) if np.isfinite(mse) else float('inf'),\n",
        "            'mae': float(mae) if np.isfinite(mae) else float('inf'),\n",
        "            'r2': float(r2) if np.isfinite(r2) else -float('inf'),\n",
        "            'predictions': predictions.flatten()\n",
        "        }\n",
        "\n",
        "    def visualize_training(self, train_losses, val_losses):\n",
        "        \"\"\"Visualize training progress\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(train_losses, label='Training Loss', alpha=0.8)\n",
        "        plt.plot(val_losses, label='Validation Loss', alpha=0.8)\n",
        "        plt.title('Training Progress')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.yscale('log')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        if len(self.performance_history) > 0:\n",
        "            plt.plot(self.performance_history)\n",
        "            plt.title('Performance History')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Validation Loss')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.yscale('log')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        if hasattr(self, 'lr_history'):\n",
        "            plt.plot(self.lr_history)\n",
        "            plt.title('Learning Rate Schedule')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Learning Rate')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "CSQwOkQ9GGBE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AIAgentDemo:\n",
        "    \"\"\"Demo class for testing the AI Agent with various scenarios\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def generate_datasets(self):\n",
        "        \"\"\"Generate multiple test datasets\"\"\"\n",
        "        datasets = {}\n",
        "\n",
        "        X1, y1 = make_regression(n_samples=600, n_features=5, n_informative=4,\n",
        "                                noise=0.1, random_state=42)\n",
        "        datasets['simple'] = (X1, y1, \"Simple Regression\")\n",
        "\n",
        "        X2, y2 = make_regression(n_samples=800, n_features=10, n_informative=8,\n",
        "                                noise=0.2, random_state=123)\n",
        "        datasets['complex'] = (X2, y2, \"Complex Regression\")\n",
        "\n",
        "        X3, y3 = make_classification(n_samples=700, n_features=8, n_informative=6,\n",
        "                                   n_classes=2, random_state=456)\n",
        "        y3 = y3.astype(float) + np.random.normal(0, 0.1, len(y3))\n",
        "        datasets['classification'] = (X3, y3, \"Classification-to-Regression\")\n",
        "\n",
        "        return datasets\n",
        "\n",
        "    def test_agent_configuration(self, config_name, X, y, **agent_params):\n",
        "        \"\"\"Test agent with specific configuration\"\"\"\n",
        "        print(f\"\\n🧪 Testing {config_name}...\")\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        default_params = {\n",
        "            'input_size': X_scaled.shape[1],\n",
        "            'hidden_layers': [32, 16],\n",
        "            'output_size': 1,\n",
        "            'learning_rate': 0.005\n",
        "        }\n",
        "        default_params.update(agent_params)\n",
        "\n",
        "        agent = AdvancedNeuralAgent(**default_params)\n",
        "\n",
        "        try:\n",
        "            train_losses, val_losses = agent.train(\n",
        "                X_scaled, y, epochs=150, batch_size=32, verbose=False)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            performance = agent.evaluate_performance(X_test, y_test)\n",
        "\n",
        "            self.agents[config_name] = agent\n",
        "            self.results[config_name] = {\n",
        "                'performance': performance,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'data_shape': X_scaled.shape\n",
        "            }\n",
        "\n",
        "            print(f\"✅ {config_name}: R²={performance['r2']:.3f}, MSE={performance['mse']:.3f}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ {config_name} failed: {str(e)[:50]}...\")\n",
        "            return False\n",
        "\n",
        "    def run_comprehensive_demo(self):\n",
        "        \"\"\"Run comprehensive testing of the AI agent\"\"\"\n",
        "        print(\"🤖 COMPREHENSIVE AI AGENT DEMO\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        datasets = self.generate_datasets()\n",
        "\n",
        "        configs = {\n",
        "            'lightweight': {'hidden_layers': [16, 8], 'learning_rate': 0.01},\n",
        "            'standard': {'hidden_layers': [32, 16], 'learning_rate': 0.005},\n",
        "            'deep': {'hidden_layers': [64, 32, 16], 'learning_rate': 0.003},\n",
        "            'wide': {'hidden_layers': [128, 64], 'learning_rate': 0.002}\n",
        "        }\n",
        "\n",
        "        success_count = 0\n",
        "        total_tests = len(datasets) * len(configs)\n",
        "\n",
        "        for dataset_name, (X, y, desc) in datasets.items():\n",
        "            print(f\"\\n📊 Dataset: {desc} - Shape: {X.shape}\")\n",
        "            print(f\"Target range: [{np.min(y):.2f}, {np.max(y):.2f}]\")\n",
        "\n",
        "            for config_name, config_params in configs.items():\n",
        "                test_name = f\"{dataset_name}_{config_name}\"\n",
        "                if self.test_agent_configuration(test_name, X, y, **config_params):\n",
        "                    success_count += 1\n",
        "\n",
        "        print(f\"\\n📈 OVERALL RESULTS: {success_count}/{total_tests} tests successful\")\n",
        "\n",
        "        if self.results:\n",
        "            self.show_best_performers()\n",
        "            self.demonstrate_agent_intelligence()\n",
        "\n",
        "    def show_best_performers(self):\n",
        "        \"\"\"Show top performing configurations\"\"\"\n",
        "        print(f\"\\n🏆 TOP PERFORMERS:\")\n",
        "\n",
        "        sorted_results = sorted(self.results.items(),\n",
        "                              key=lambda x: x[1]['performance']['r2'],\n",
        "                              reverse=True)\n",
        "\n",
        "        for i, (name, result) in enumerate(sorted_results[:5]):\n",
        "            perf = result['performance']\n",
        "            print(f\"{i+1}. {name}: R²={perf['r2']:.3f}, MSE={perf['mse']:.3f}, MAE={perf['mae']:.3f}\")\n",
        "\n",
        "    def demonstrate_agent_intelligence(self):\n",
        "        \"\"\"Demonstrate advanced AI capabilities\"\"\"\n",
        "        if not self.agents:\n",
        "            return\n",
        "\n",
        "        print(f\"\\n🧠 INTELLIGENCE DEMONSTRATION:\")\n",
        "\n",
        "        best_name = max(self.results.keys(),\n",
        "                       key=lambda x: self.results[x]['performance']['r2'])\n",
        "        best_agent = self.agents[best_name]\n",
        "\n",
        "        print(f\"Using best agent: {best_name}\")\n",
        "\n",
        "        print(f\"💾 Memory capacity: {len(best_agent.memory)} experiences\")\n",
        "\n",
        "        dummy_input = np.random.randn(3, best_agent.layers[0]['weights'].shape[0])\n",
        "        conservative_decisions = best_agent.make_decision(dummy_input, exploration_rate=0.0)\n",
        "        exploratory_decisions = best_agent.make_decision(dummy_input, exploration_rate=0.3)\n",
        "\n",
        "        print(f\"🎯 Decision making:\")\n",
        "        print(f\"   Conservative: {conservative_decisions.flatten()[:3]}\")\n",
        "        print(f\"   Exploratory:  {exploratory_decisions.flatten()[:3]}\")\n",
        "\n",
        "        if len(best_agent.performance_history) > 10:\n",
        "            initial_perf = np.mean(best_agent.performance_history[:5])\n",
        "            final_perf = np.mean(best_agent.performance_history[-5:])\n",
        "            improvement = ((initial_perf - final_perf) / initial_perf) * 100\n",
        "            print(f\"📊 Learning improvement: {improvement:.1f}%\")\n",
        "\n",
        "        total_params = sum(layer['weights'].size + layer['bias'].size\n",
        "                          for layer in best_agent.layers)\n",
        "        print(f\"🔧 Network complexity: {total_params} parameters\")\n",
        "\n",
        "        return best_agent"
      ],
      "metadata": {
        "id": "FM5Z7iEvG-3p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_quick_demo():\n",
        "    \"\"\"Quick demo for immediate testing\"\"\"\n",
        "    print(\"🚀 QUICK AI AGENT DEMO\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    X, y = make_regression(n_samples=500, n_features=6, noise=0.15, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    print(f\"Dataset: {X_scaled.shape[0]} samples, {X_scaled.shape[1]} features\")\n",
        "\n",
        "    agent = AdvancedNeuralAgent(\n",
        "        input_size=X_scaled.shape[1],\n",
        "        hidden_layers=[24, 12],\n",
        "        output_size=1,\n",
        "        learning_rate=0.008\n",
        "    )\n",
        "\n",
        "    print(\"Training agent...\")\n",
        "    train_losses, val_losses = agent.train(X_scaled, y, epochs=100, verbose=False)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "    performance = agent.evaluate_performance(X_test, y_test)\n",
        "\n",
        "    print(f\"\\n✅ RESULTS:\")\n",
        "    print(f\"R² Score: {performance['r2']:.3f}\")\n",
        "    print(f\"MSE: {performance['mse']:.3f}\")\n",
        "    print(f\"MAE: {performance['mae']:.3f}\")\n",
        "\n",
        "    agent.visualize_training(train_losses, val_losses)\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "qNz242O0GIuU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh5hcV8JBmIW",
        "outputId": "92f5b014-7fb4-40d1-d65b-6b71cd691771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose demo type:\n",
            "1. Quick Demo (fast)\n",
            "2. Comprehensive Demo (detailed)\n",
            "🤖 COMPREHENSIVE AI AGENT DEMO\n",
            "============================================================\n",
            "\n",
            "📊 Dataset: Simple Regression - Shape: (600, 5)\n",
            "Target range: [-480.99, 497.81]\n",
            "\n",
            "🧪 Testing simple_lightweight...\n",
            "✅ simple_lightweight: R²=0.993, MSE=212.513\n",
            "\n",
            "🧪 Testing simple_standard...\n",
            "✅ simple_standard: R²=0.992, MSE=216.751\n",
            "\n",
            "🧪 Testing simple_deep...\n",
            "✅ simple_deep: R²=0.987, MSE=381.305\n",
            "\n",
            "🧪 Testing simple_wide...\n",
            "✅ simple_wide: R²=0.995, MSE=129.706\n",
            "\n",
            "📊 Dataset: Complex Regression - Shape: (800, 10)\n",
            "Target range: [-578.88, 598.74]\n",
            "\n",
            "🧪 Testing complex_lightweight...\n",
            "✅ complex_lightweight: R²=0.991, MSE=329.505\n",
            "\n",
            "🧪 Testing complex_standard...\n",
            "✅ complex_standard: R²=0.981, MSE=663.555\n",
            "\n",
            "🧪 Testing complex_deep...\n",
            "✅ complex_deep: R²=0.983, MSE=598.317\n",
            "\n",
            "🧪 Testing complex_wide...\n",
            "✅ complex_wide: R²=0.990, MSE=355.429\n",
            "\n",
            "📊 Dataset: Classification-to-Regression - Shape: (700, 8)\n",
            "Target range: [-0.31, 1.24]\n",
            "\n",
            "🧪 Testing classification_lightweight...\n",
            "✅ classification_lightweight: R²=0.718, MSE=0.070\n",
            "\n",
            "🧪 Testing classification_standard...\n",
            "✅ classification_standard: R²=0.653, MSE=0.087\n",
            "\n",
            "🧪 Testing classification_deep...\n",
            "✅ classification_deep: R²=0.677, MSE=0.081\n",
            "\n",
            "🧪 Testing classification_wide...\n",
            "✅ classification_wide: R²=0.668, MSE=0.083\n",
            "\n",
            "📈 OVERALL RESULTS: 12/12 tests successful\n",
            "\n",
            "🏆 TOP PERFORMERS:\n",
            "1. simple_wide: R²=0.995, MSE=129.706, MAE=8.642\n",
            "2. simple_lightweight: R²=0.993, MSE=212.513, MAE=10.417\n",
            "3. simple_standard: R²=0.992, MSE=216.751, MAE=11.528\n",
            "4. complex_lightweight: R²=0.991, MSE=329.505, MAE=14.380\n",
            "5. complex_wide: R²=0.990, MSE=355.429, MAE=14.698\n",
            "\n",
            "🧠 INTELLIGENCE DEMONSTRATION:\n",
            "Using best agent: simple_wide\n",
            "💾 Memory capacity: 0 experiences\n",
            "🎯 Decision making:\n",
            "   Conservative: [ 1.23146432 -1.12526418 -1.70892481]\n",
            "   Exploratory:  [ 1.02293598 -1.25920221 -1.57835509]\n",
            "📊 Learning improvement: 99.5%\n",
            "🔧 Network complexity: 9089 parameters\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Choose demo type:\")\n",
        "    print(\"1. Quick Demo (fast)\")\n",
        "    print(\"2. Comprehensive Demo (detailed)\")\n",
        "\n",
        "    demo = AIAgentDemo()\n",
        "    best_agent = demo.run_comprehensive_demo()"
      ]
    }
  ]
}